{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuring the Nodes\n",
    "\n",
    "Server = main node and worker nodes were configured according to the script below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#on the server:\n",
      "\n",
      " yum install nfs-utils rpcbind\n",
      "\n",
      " # Used to enable nfs starting at the boot of the machine\n",
      " systemctl enable nfs-server\n",
      " systemctl enable rpcbind     # starts the boot\n",
      " systemctl enable nfs-lock\n",
      " systemctl enable nfs-idmap \n",
      " systemctl start rpcbind      # starting the service immediately\n",
      " systemctl start nfs-server\n",
      " systemctl start nfs-lock\n",
      " systemctl start nfs-idmap\n",
      "\n",
      " \n",
      " systemctl status nfs   ### Checking that everything is fine 'system control'\n",
      "# you should see:\n",
      "#  Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)\n",
      "#    Active: active (exited) since Thu 2021-02-18 16:33:02 UTC; 1min 46s ago\n",
      "#   Process: 16053 ExecStartPost=/bin/sh -c if systemctl -q is-active gssproxy; then systemctl reload gssproxy ; fi (code=exited, status=0/SUCCESS)\n",
      "#   Process: 16035 ExecStart=/usr/sbin/rpc.nfsd $RPCNFSDARGS (code=exited, status=0/SUCCESS)\n",
      "#   Process: 16033 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS)\n",
      "#  Main PID: 16035 (code=exited, status=0/SUCCESS)\n",
      "#    CGroup: /system.slice/nfs-server.service\n",
      "\n",
      "## chacking for deamon:\n",
      "ps auxwf\n",
      "# large printout on terminal - scoll until you find;\n",
      "# root     16038  0.0  0.0      0     0 ?        S<   16:33   0:00  \\_ [nfsd4_callbacks]\n",
      "# root     16039  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [lockd]\n",
      "# root     16044  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root     16045  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root     16046  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root     16047  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root     16048  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root     16049  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]             ########## nfsd :)\n",
      "# root     16050  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root     16051  0.0  0.0      0     0 ?        S    16:33   0:00  \\_ [nfsd]\n",
      "# root         1  0.0  0.2 128168  8776 ?        Ss   15:20   0:03 /usr/lib/system\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "# Edit /etc/exports file says which fs are exported to which machines\n",
      "####################################################################################################\n",
      " vim /etc/exports\n",
      "      # insert:\n",
      "      /data  <destination_host IP - USE the private IP of the WN instance >(rw,sync,no_wdelay)\n",
      "      # eg:\n",
      "      /data 172.31.89.57(rw,sync,no_wdelay)\n",
      "\n",
      "cat /etc/exports\n",
      "      /data  <destination_host IP - USE the private IP>(rw,sync,no_wdelay)\n",
      " \n",
      "####################################################################################################\n",
      "# Cmd to read exports file and export fs listed there\n",
      "####################################################################################################\n",
      "exportfs -r\n",
      "# be sure that no errors are printed!\n",
      "\n",
      "# run 'exportfs' and check that the ip is the same as previously entered :)\n",
      "# [root@ip-172-31-90-23 ~]# exportfs \n",
      "# /data         \t172.31.89.57\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "#On the client: the worker node\n",
      "####################################################################################################\n",
      "yum install vim\n",
      "\n",
      " yum install nfs-utils\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      " mount -t nfs -o ro,nosuid <your_private_server_ip_from_MAIN_node>:/data2 /data  # I named my master dir /data2 ...\n",
      " # braking it down: # mout -t = type, nfs -o = options, ro = readonly \n",
      "\n",
      " ll /data/  # now we can see how the data2 was imported to data\n",
      "\n",
      " # We transformed our DAS into as NAS\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "# Lets edit the /etc/fstab --> that way the FS will be automatically mounted after reboot :)\n",
      "####################################################################################################\n",
      "\n",
      " cat /etc/fstab\n",
      "<MAIN_SERVER_PRIVATE_IP>:/data2 /data   nfs defaults        0 0\n",
      "\n",
      "# We un mount \n",
      "umount /data\n",
      "\n",
      "# Then we mount all from fstab file to see if it worked:\n",
      "\n",
      "mount -a"
     ]
    }
   ],
   "source": [
    "cat ../pastebins/05L_vid_p2_installconfigure_an_NFS_serverclient.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Install HT condor machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#INSTALL DEPENDENCIES\n",
      "yum install wget\n",
      "wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n",
      "yum localinstall epel-release-latest-7.noarch.rpm\n",
      "yum clean all\n",
      "\n",
      "\n",
      "#INSTALL CONDOR REPOs and PACKAGES\n",
      "wget http://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-stable-rhel7.repo\n",
      "cp htcondor-stable-rhel7.repo /etc/yum.repos.d/\n",
      "wget http://htcondor.org/yum/RPM-GPG-KEY-HTCondor\n",
      "rpm --import RPM-GPG-KEY-HTCondor\n",
      "yum install condor-all\n",
      "\n",
      "#CONDOR BASIC CONFIGURATION\n",
      "cd\n",
      "vim /etc/condor/condor_config\n",
      "systemctl status condor \n",
      "systemctl start condor \n",
      "systemctl enable condor\n",
      "systemctl status condor \n",
      "ps -aux | grep condor\n",
      "\n",
      "# END - SOME HINTS BELOW\n",
      "1) Security Group must allow tcp for ports 0 - 65535 from the same security group, i.e.:\n",
      " All TCP    TCP      0 - 65535     sg-008742ba0467986fe (aws_condor)\n",
      "Security group must allow ping from the same security group, i.e.:\n",
      " All    ICMP-IPv4   All    N/A     sg-008742ba0467986fe (aws_condor)\n",
      "Security group must allow ssh on port 22 from everywhere as ususal\n",
      "\n",
      "##################################\n",
      "\n",
      "\n",
      "#-------------------------------------\n",
      "# In the config file add at the end\n",
      "# the most important variable is the CONDOR_HOST running the master\n",
      "\n",
      "# ADD the following lines to your condor_config file\n",
      "# CHANGE THE FOLLOWING IP TO YOUR MASTER IP\n",
      "CONDOR_HOST = __put here the master Private IP address, i.e.: 172.31.25.191 ___\n",
      "\n",
      "# on the master\n",
      "DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, STARTD, SCHEDD\n",
      "\n",
      "# #on the nodes\n",
      "# #DAEMON_LIST = MASTER, STARTD\n",
      "\n",
      "\n",
      "HOSTALLOW_READ = *\n",
      "HOSTALLOW_WRITE = *\n",
      "HOSTALLOW_ADMINISTRATOR = *\n",
      "#---------------------------------------------\n",
      "\n",
      "\n",
      "#Depending on your flavor of Unix. On a central manager machine that can submit jobs as well as execute them, there will be processes for:\n",
      "\n",
      "    condor_master\n",
      "    condor_collector\n",
      "    condor_negotiator\n",
      "    condor_startd\n",
      "    condor_schedd\n",
      "\n",
      "On a central manager machine that does not submit jobs nor execute them, there will be processes for:\n",
      "\n",
      "    condor_master\n",
      "    condor_collector\n",
      "    condor_negotiator\n",
      "\n",
      "For a machine that only submits jobs, there will be processes for:\n",
      "\n",
      "    condor_master\n",
      "    condor_schedd\n",
      "\n",
      "For a machine that only executes jobs, there will be processes for:\n",
      "    condor_master\n",
      "    condor_startd\n",
      "\n",
      "USE PRIVATE IPs!!\n",
      "[root@aws_ui1 ~]# cat   /etc/condor/condor_config\n",
      "######################################################################\n",
      "##\n",
      "##  condor_config\n",
      "##\n",
      "##  This is the global configuration file for condor. This is where\n",
      "##  you define where the local config file is. Any settings\n",
      "##  made here may potentially be overridden in the local configuration\n",
      "##  file.  KEEP THAT IN MIND!  To double-check that a variable is\n",
      "##  getting set from the configuration file that you expect, use\n",
      "##  condor_config_val -v <variable name>\n",
      "##\n",
      "##  condor_config.annotated is a more detailed sample config file\n",
      "##\n",
      "##  Unless otherwise specified, settings that are commented out show\n",
      "##  the defaults that are used if you don't define a value.  Settings\n",
      "##  that are defined here MUST BE DEFINED since they have no default\n",
      "##  value.\n",
      "##\n",
      "######################################################################\n",
      "\n",
      "##  Where have you installed the bin, sbin and lib condor directories?\n",
      "RELEASE_DIR = /usr\n",
      "\n",
      "##  Where is the local condor directory for each host?  This is where the local config file(s), logs and\n",
      "##  spool/execute directories are located. this is the default for Linux and Unix systems.\n",
      "LOCAL_DIR = /var\n",
      "\n",
      "##  Where is the machine-specific local config file for each host?\n",
      "LOCAL_CONFIG_FILE = /etc/condor/condor_config.local\n",
      "##  If your configuration is on a shared file system, then this might be a better default\n",
      "#LOCAL_CONFIG_FILE = $(RELEASE_DIR)/etc/$(HOSTNAME).local\n",
      "##  If the local config file is not present, is it an error? (WARNING: This is a potential security issue.)\n",
      "REQUIRE_LOCAL_CONFIG_FILE = false\n",
      "\n",
      "##  The normal way to do configuration with RPMs is to read all of the\n",
      "##  files in a given directory that don't match a regex as configuration files.\n",
      "##  Config files are read in lexicographic order.\n",
      "LOCAL_CONFIG_DIR = /etc/condor/config.d\n",
      "#LOCAL_CONFIG_DIR_EXCLUDE_REGEXP = ^((\\..*)|(.*~)|(#.*)|(.*\\.rpmsave)|(.*\\.rpmnew))$\n",
      "\n",
      "##  Use a host-based security policy. By default CONDOR_HOST and the local machine will be allowed\n",
      "use SECURITY : HOST_BASED\n",
      "##  To expand your condor pool beyond a single host, set ALLOW_WRITE to match all of the hosts\n",
      "#ALLOW_WRITE = *.cs.wisc.edu\n",
      "##  FLOCK_FROM defines the machines that grant access to your pool via flocking. (i.e. these machines can join your pool).\n",
      "#FLOCK_FROM =\n",
      "##  FLOCK_TO defines the central managers that your schedd will advertise itself to (i.e. these pools will give matches to your schedd).\n",
      "#FLOCK_TO = condor.cs.wisc.edu, cm.example.edu\n",
      "\n",
      "##--------------------------------------------------------------------\n",
      "## Values set by the rpm patch script:\n",
      "##--------------------------------------------------------------------\n",
      "\n",
      "## For Unix machines, the path and file name of the file containing\n",
      "## the pool password for password authentication.\n",
      "#SEC_PASSWORD_FILE = $(LOCAL_DIR)/lib/condor/pool_password\n",
      "\n",
      "##  Pathnames\n",
      "RUN     = $(LOCAL_DIR)/run/condor\n",
      "LOG     = $(LOCAL_DIR)/log/condor\n",
      "LOCK    = $(LOCAL_DIR)/lock/condor\n",
      "SPOOL   = $(LOCAL_DIR)/lib/condor/spool\n",
      "EXECUTE = $(LOCAL_DIR)/lib/condor/execute\n",
      "BIN     = $(RELEASE_DIR)/bin\n",
      "LIB = $(RELEASE_DIR)/lib64/condor\n",
      "INCLUDE = $(RELEASE_DIR)/include/condor\n",
      "SBIN    = $(RELEASE_DIR)/sbin\n",
      "LIBEXEC = $(RELEASE_DIR)/libexec/condor\n",
      "SHARE   = $(RELEASE_DIR)/share/condor\n",
      "\n",
      "PROCD_ADDRESS = $(RUN)/procd_pipe\n",
      "\n",
      "JAVA_CLASSPATH_DEFAULT = $(SHARE) $(SHARE)/scimark2lib.jar .\n",
      "\n",
      "SSH_TO_JOB_SSHD_CONFIG_TEMPLATE = /etc/condor/condor_ssh_to_job_sshd_config_template\n",
      "\n",
      "##  Install the minicondor package to run HTCondor on a single node\n",
      "#\n",
      "\n",
      "# CHANGE THE FOLLOWING IP TO YOUR MASTER IP\n",
      "CONDOR_HOST = 172.31.89.103\n",
      "\n",
      "# on the master\n",
      "DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, STARTD, SCHEDD\n",
      "\n",
      "#on the nodes\n",
      "#DAEMON_LIST = MASTER, STARTD\n",
      "\n",
      "HOSTALLOW_READ = *\n",
      "HOSTALLOW_WRITE = *\n",
      "HOSTALLOW_ADMINISTRATOR = *"
     ]
    }
   ],
   "source": [
    "# cat ../pastebins/06L_p1_condor_install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up master:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vim /etc/condor/condor_config # jump to line 36 well add the line 40\n",
    "#-------------------------------------\n",
    "# In the config file add at the end\n",
    "# the most important variable is the CONDOR_HOST running the master\n",
    "\n",
    "# ADD the following lines to your condor_config file\n",
    "# CHANGE THE FOLLOWING IP TO YOUR MASTER IP\n",
    "CONDOR_HOST = __put here the master Private IP address, i.e.: 172.31.25.191 ___ # add to Main and ALL NODES\n",
    "# Condor host is the main!\n",
    "# ONLY on the master\n",
    "DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, STARTD, SCHEDD\n",
    "\n",
    "HOSTALLOW_READ = *\n",
    "HOSTALLOW_WRITE = *\n",
    "HOSTALLOW_ADMINISTRATOR = *\n",
    "#---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up stateless worker nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vim /etc/condor/condor_config # jump to line 36 well add the line 40\n",
    "#-------------------------------------\n",
    "# In the config file add at the end\n",
    "# the most important variable is the CONDOR_HOST running the master\n",
    "\n",
    "# ADD the following lines to your condor_config file\n",
    "# CHANGE THE FOLLOWING IP TO YOUR MASTER IP\n",
    "CONDOR_HOST = __put here the master Private IP address, i.e.: 172.31.25.191 ___ # add to Main and ALL NODES\n",
    "# Condor host is the main!\n",
    "\n",
    "# #ONLY on the nodes\n",
    "DAEMON_LIST = MASTER, STARTD\n",
    "\n",
    "HOSTALLOW_READ = *\n",
    "HOSTALLOW_WRITE = *\n",
    "HOSTALLOW_ADMINISTRATOR = *\n",
    "#---------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying /etc/exportfs\n",
    "\n",
    "* By adding a line that is valid for all worker nodes:\n",
    "* This is done by exploiting the CIDIR notation \n",
    "* All wn are in the SAME subnet!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/data2 <first_5_digits_of_private_ip/16>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
